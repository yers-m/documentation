# Parameter Estimation

* parameter values define “goodness” of the model

## Objective Function Value
* represents the goodness of fit [[2013_Mould]](https://doi.org/10.1038/psp.2013.14)
* proportional to minus 2 times the log likelihood (-2LL)
* extended least squares **ELS** objective function [[1980_Sheiner]](https://doi.org/10.1007/bf01060053)
* the preference is given to lower OFV [[2013_Mould]](https://doi.org/10.1038/psp.2013.14)
    * by iterative “hill climbing” procedure to find the lowest OFV, or minima, within a given search space. [[2008_Cambini]](https://books.google.de/books?hl=en&lr=&id=JEcwQgngoE8C&oi=fnd&pg=PA1&dq=Cambini,+A.,+%26+Martein,+L.+(2008).+Generalized+convexity+and+optimization:+Theory+and+applications+(Vol.+616).+Springer+Science+%26+Business+Media.&ots=bah69g9QPg&sig=Kwa2904XtX65SkbLCrqmrqa23W4&redir_esc=y#v=onepage&q&f=false)
* Initial parameter estimates have an important role [[2015_Sale]](https://doi.org/10.1111/bcp.12179)
    * estimation can be “trapped” gradient search in local OFV minima, and “mask” the global minimum 

![](./screenshots/parameter_estimation.png)
 *Figure was adopted from [[2015_Sale]](https://doi.org/10.1111/bcp.12179).*

---

## Algorithms
### Gradient-based algorithms
* Taylor series approximations for numerical solution of the likelihood function
* 

### First-Order Conditional Estimation algorithm - <kbd>**FOCE**</kbd>
* linearised by conditioning on the individual etas [[2014_Owen]](https://doi.org/10.1038%2Fpsp.2014.51) [[2014_Johansson]](https://doi.org/10.1007/s10928-014-9359-z)
### FOCE with interaction - <kbd>**FOCEI**</kbd>
* considering the interaction between ε and η [[2014_Owen]](https://doi.org/10.1038%2Fpsp.2014.51)

### <kbd>**LAPLACE**</kbd>
* [[2014_Johansson]](https://doi.org/10.1007/s10928-014-9359-z)
* second-order approximation
* only gradient-based estimation method 
* can be used for categorical data
* can be used to consider observations below LLOQ
* more unstable than e.g. FOCE

### Stochastic Approximation Expectation Maximisation - <kbd>**SAEM**</kbd>
* [[2007_Bauer]](https://doi.org/10.1208/aapsj0901007)
* step E: stochastic approximation
* step M: maximises the expected likelihood
* includes one burn-in and one accumulation phase [[2009_Beal]](https://www.semanticscholar.org/paper/NONMEM-User%E2%80%99s-Guides.-(1989%E2%80%932009)-Beal-Boeckmann/1964357daa9975ac959840262a810b2e0b39c8f4)
    * burn-in: approximation is done on few samples per individual, and maximised and the process is repeated until the estimates have stabilised
    * accumulation: the individual random-effects are sampled and averaged together
### IMPortance sampling - <kbd>**IMP**</kbd>
* [[2007_Bauer]](https://doi.org/10.1208/aapsj0901007)
    * step E: Monte-Carlo integration to assess the conditional mean and variance of $η_i$
    * step M: maximises the expected likelihood
* objective function is commonly generated by few iterations of IMP for the final parameter estimates.

> Note: [[2014_Johansson]](https://doi.org/10.1007/s10928-014-9359-z) <br>
> **step E** *expectation* evaluates the expected likelihood with respect to the conditional distribution of $η_i$ based on the current parameter estimates and the observed data; <br>
> **step M** *maximisation* maximises the expected likelihood (from step E) to generate new parameter estimates.

---

